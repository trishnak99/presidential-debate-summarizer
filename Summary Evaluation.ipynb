{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMM_DIR = 'data/summaries'\n",
    "TRUMP_GOLD = 'trump_gold'\n",
    "TRUMP_SUMM = ['trump_baseline', 'trump_lexrank', 'trump_textrank', 'trump_bert']\n",
    "BIDEN_GOLD = 'biden_gold'\n",
    "BIDEN_SUMM = ['biden_baseline', 'biden_lexrank', 'biden_textrank', 'biden_bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation on Trump Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating ROUGE scores for trump_baseline...\n",
      "{'rouge1': Score(precision=0.7327586206896551, recall=0.9309400669303316, fmeasure=0.8200455580865603),\n",
      " 'rouge2': Score(precision=0.6045508982035929, recall=0.7681071211199026, fmeasure=0.6765849081892508),\n",
      " 'rougeL': Score(precision=0.18318965517241378, recall=0.2327350167325829, fmeasure=0.20501138952164008)}\n",
      "\n",
      "Evaluating ROUGE scores for trump_lexrank...\n",
      "{'rouge1': Score(precision=0.8148714810281518, recall=0.8101612412534226, fmeasure=0.812509534706331),\n",
      " 'rouge2': Score(precision=0.578818487909397, recall=0.5754716981132075, fmeasure=0.5771402411109416),\n",
      " 'rougeL': Score(precision=0.3812729498164015, recall=0.37906905993306966, fmeasure=0.3801678108314264)}\n",
      "\n",
      "Evaluating ROUGE scores for trump_textrank...\n",
      "{'rouge1': Score(precision=0.7197937131630648, recall=0.8916945543048372, fmeasure=0.7965756216877293),\n",
      " 'rouge2': Score(precision=0.5708671088184721, recall=0.707242848447961, fmeasure=0.6317792578496669),\n",
      " 'rougeL': Score(precision=0.3835952848722986, recall=0.4752053544265287, fmeasure=0.42451420029895365)}\n",
      "\n",
      "Evaluating ROUGE scores for trump_bert...\n",
      "{'rouge1': Score(precision=0.5782178217821782, recall=0.9771828414968056, fmeasure=0.7265324587197466),\n",
      " 'rouge2': Score(precision=0.5127835794022326, recall=0.8667072428484479, fmeasure=0.6443438914027149),\n",
      " 'rougeL': Score(precision=0.32925292529252925, recall=0.5564344386979008, fmeasure=0.4137073060393576)}\n"
     ]
    }
   ],
   "source": [
    "gold_f = os.path.join(SUMM_DIR, TRUMP_GOLD) + '.txt'\n",
    "with open(gold_f, 'r') as f:\n",
    "    gold = f.read()\n",
    "\n",
    "for summ in TRUMP_SUMM:\n",
    "    system_f = os.path.join(SUMM_DIR, summ) + '.txt'\n",
    "    with open(system_f, 'r') as f:\n",
    "        system = f.read()\n",
    "    \n",
    "    print('\\nEvaluating ROUGE scores for {}...'.format(summ))\n",
    "    scores = scorer.score(gold, system)\n",
    "    pprint(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation on Biden Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating ROUGE scores for biden_baseline...\n",
      "{'rouge1': Score(precision=0.7184124591811103, recall=0.9407894736842105, fmeasure=0.8146987608602763),\n",
      " 'rouge2': Score(precision=0.6150753768844222, recall=0.805528134254689, fmeasure=0.6975352614332526),\n",
      " 'rougeL': Score(precision=0.20120572720422006, recall=0.26348684210526313, fmeasure=0.22817262498219626)}\n",
      "\n",
      "Evaluating ROUGE scores for biden_lexrank...\n",
      "{'rouge1': Score(precision=0.7854785478547854, recall=0.7828947368421053, fmeasure=0.7841845140032948),\n",
      " 'rouge2': Score(precision=0.575107296137339, recall=0.57321487331359, fmeasure=0.5741595253790375),\n",
      " 'rougeL': Score(precision=0.26732673267326734, recall=0.26644736842105265, fmeasure=0.26688632619439867)}\n",
      "\n",
      "Evaluating ROUGE scores for biden_textrank...\n",
      "{'rouge1': Score(precision=0.724193118164844, recall=0.8930921052631579, fmeasure=0.7998232434821034),\n",
      " 'rouge2': Score(precision=0.5909818569903948, recall=0.7288581770319184, fmeasure=0.6527184322970384),\n",
      " 'rougeL': Score(precision=0.2742064550546813, recall=0.3381578947368421, fmeasure=0.3028428339961703)}\n",
      "\n",
      "Evaluating ROUGE scores for biden_bert...\n",
      "{'rouge1': Score(precision=0.5504939626783754, recall=0.9898026315789473, fmeasure=0.7075005878203622),\n",
      " 'rouge2': Score(precision=0.5139981701738335, recall=0.9243172096084238, fmeasure=0.6606302916274694),\n",
      " 'rougeL': Score(precision=0.22484449323088182, recall=0.4042763157894737, fmeasure=0.2889724900070539)}\n"
     ]
    }
   ],
   "source": [
    "gold_f = os.path.join(SUMM_DIR, BIDEN_GOLD) + '.txt'\n",
    "with open(gold_f, 'r') as f:\n",
    "    gold = f.read()\n",
    "\n",
    "for summ in BIDEN_SUMM:\n",
    "    system_f = os.path.join(SUMM_DIR, summ) + '.txt'\n",
    "    with open(system_f, 'r') as f:\n",
    "        system = f.read()\n",
    "    \n",
    "    print('\\nEvaluating ROUGE scores for {}...'.format(summ))\n",
    "    scores = scorer.score(gold, system)\n",
    "    pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
